---
output:
  pdf_document: default
  html_document: default
---
MODERN STATISTICAL METHODS WITH R ~ TEST 1

Michal Zieff

In this test, attempt to answer the following research question: what are the strongest predictors of juvenile conviction? My aim is build a model that predicts whether or not a child is convicted.  

QUESTION 1: Create a workable dataset 

An initial look at the codebook and papers suggests that we will be dealing with a large number of variables. We start by importing the two .sav datasets into R. First, though, we should load all packages that we will need for this analysis. 

```{r}
library(pacman)
p_load(tidyverse, haven, magrittr, psych, janitor, labelled, car, knitr)
```

We use a function from package "haven" to load the two spss files into R. 

```{r}
del.dat <- read_spss("data/Cambridge_delinquency.sav")
con.dat <- read_spss("data/conviction_data.sav")
```

Upon inspection of the con.dat dataset, it looks like we need to change it into wide format. 

```{r}
con_wide.dat <- tidyr::spread(con.dat, key = agecat, value = convicted) #want columns to represent the different age categories, and the rows to list the number of convictions at each age range. 
```

N is now 411 in both datasets, which is good. There is no existing "matching" criterion on which to join the two datasets (e.g., the del.dat dataset does not specify participant ID number). It appears that to use the merge or join functions, there has to be at least one matching variable. Hence, I will simply join the datasets on the assumption that the row numbers in both datasets represent the same person. To do this, I am going to create a participant ID variable (identical to that in the con_wide.dat dataset), add it to the del.dat dataset, and then match del.dat and con_wide.dat on that variable. The column with the ID variable in the con_wide.dat dataset is simply a vector of numbers 1:411. So, if I create a similar vector and add it as a variable to del.dat, we now have a matching variable. 

```{r}

icpsr_seq_id_number <- 1:411 #create a vector containing numbers 1 - 411

#Now, join it as a column to del.dat

del_id.dat <- cbind(del.dat, icpsr_seq_id_number)

#Join the two datasets

joined.dat <- left_join(del_id.dat, con_wide.dat, by = "icpsr_seq_id_number")

```

The del.dat dataset still presents some problems. First, it contains 870 variables. We need to decide on a strategy to weed through this dataset, with the aim of creating a clean and manageable data frame with which to build our model. 

As an outcome variable, I think it makes sense to predict juvenile conviction, rather than adult conviction, as this is central to our research question and aim. In the dataset, juvenile delinquency is coded as both a categorical and continuous variable. I will include both the continuous variable "# of juvenile convictions" and the categorical variable "convicted as a minor - yes or no?" for now. There has to be a way to limit ourselves as to the variables we are going to look at, or we'll be here forever. Because I would like to determine the childhood factors that predict juvenile conviction, I will not include variables that were recorded past the age of 12.

Within those 310 variables, I will begin with those that make the most sense to include (based on studies that have come from the data, and other research that looked at outcomes of these predictors). I present them below categorised by theme, with an indication of how the scores/responses are coded: 

BIRTH RELATED/TEMPERAMENT/PERSONALITY:

Var 67 - fretful baby. 1 = not fretful, 2 = fretful.
Var 113 - presence of abnormality from birthing records (bw, confinement, preg). 0 = not known, 1 = no abnormality, 2 = abnormality present. 
Var 117 - outgoing or withdrawn (evaluation of child's temperament). 0 = unknown, 1 = at ease/outgoing, 2 = normal, 3 = rather shy/definitely withdrawn. 
Var 54 - combined conduct disorder. Conduct disorder of boy (as rated by interviewer) and teacher's rating of bad behaviour. Coding 1 = good (no conduct disorder) to 5 = bad/severe conduct disorder. Not sure whether this is the right category for this variable, because it encompasses a bit of everything, but we will leave it here for now. 


HOME/FAMILY RELATED

Var 142 - social handicap. Composite variable of seven individual variables (reliance of social agencies, housing conditions - interior and general, a marker of socioeconomic status, income of family, large family, and physical neglect of boy). If none = 1, if 1-3 of these things, response coded as 2, if 4-7 of these things, the response coded as 3. 
Var 123 - physical neglect of boy. Want to include this as its own variable. 0 = unknown, 1 = neglect absent, 2 - neglect present. 
Var 134 broken home (separation of biological parents) before age 10. 1 = no borken home, 2 = broken due to death only, 3 = broken home
Var 178 - attitude of father combined - composite variable (attitude, discipline, and discipline quality). 0 = unknown, 1 = good, 2 = good average, 3 = poor average, 4 = poor. 
Var 179 - attitude of mother combined - composite variable (same as for paternal above).
Var 91 - parents interested in child's education. 0 = unknown, 1 = very interested, 2 = average, 3 = not interested. 

SCHOOL/SOCIAL-RELATED:

Var 105 - number of friends of boy. 0 = not known, 1 = makes many friends, 2 = has average number, 3 = has few or no friends 
Var 125 - peer rating popularity. 0 = unknown, 1 = popular, 2 = average popular, 3 = average unpopular, 4 = unpopular. 
Var 155 - teacher rating poor attendance. 0 = unknown, 1 = satisfactory attendance, 2 = absenteeism mentioned. 
Var 297 - teacher rating difficult to discipline. 0 = unknown, 1 = not difficult, 2 = difficult. 

IQ

Var 119 progressive matrices (a measure of IQ). 1 = highest scoring bracket, and 4 = lowest scoring bracket.
Var 158 - verbal comprehension quotient (another cognitive measure). 1 = highest bracket, and 4 = lowst scoring bracket. 


OTHER RISK FACTORS

Var 300 - criminal record of parents. 1 = no parent convicted, 2 = convicted as juvenile only, 3 = once only as adult, 4 = twice as adult, 5 = three times as an adult, 6 = four or more times as an adult. 
Var 301 - delinquent older sibling. 1 = no older sibling convicted, 2 = older sibling convicted. 

Need to make a new dataset including the above predictors, plus our chosen outcome variable:

```{r}
convict.dat <- select(joined.dat, icpsr_seq_id_number, convicted_as_juvenile, v28, v67, v113, v117, v54, v142, v123, v134, v178, v179, v91, v105, v125, v155, v297, v119, v158, v300, v301) #select variables from the large dataset to create a new dataset

convict.dat$convicted_as_juvenile <- as.factor(convict.dat$convicted_as_juvenile) #changing the categorical predictor to a factor (originally numeric). 

str(convict.dat$convicted_as_juvenile) #check

```

QUESTION 2: Explore the data

First, we should do a quick check for impossible/unreasonable values in the dataset. We have a clear idea of how scores on the variables should range, and any value that does not make sense in this context should be removed. We can also kill two birds with one stone if we use the describeBy function in package psych, which will allow us a rudimentary overview into any possible between-group differences. 


```{r}
describe(convict.dat)
```

As a start, it would be useful to see how many boys in our sample ended up committing crimes at all, and if so, then how many. Let's look at the spread of our predictor variable. 

```{r}

convict.dat$v28 <- as.numeric(convict.dat$v28) #changing variable type to numeric. 

qplot(convict.dat$v28, geom = "histogram", binwidth = 0.5, main = "Histogram showing spread of convictions", xlab = "Number of juvenile convictions") #make a quick plot using ggplot2 to check spread of convictions in the sample
```

We have a very skewed distribution, which makes sense to us given what the variable is looking us. Most of the boys were never convicted of any crime. Just under 50 boys committed one crime, around 20 were convicted of two or three crimes, and only about 15 or so were convicted of four or more crimes. An option may be to perform a Box-Cox transformation. This would, however, make for a trickier (and less meaningful) interpretation, that is, we would be predicting number of convictions to the power of x, rather than number of convictions. We can leave it for now. 

Due to our large number of predictors, it does not seem prudent to inspect the spread of all of them. We can use the pairs.panel function from package psych for an overview on the relationships between the predictors and the outcome variable. 

```{r}

convict.dat %>% 
  select(v28, v67, v113, v117, v54, v142, v123, v134, v178, v179) %>% 
  #Dividing the variables up into 2 chunks to make it easier to read.
  #Selecting the first batch here.
  pairs.panels()
```
At a quick glance, it does not look like there is much of a relationship between number of juvenile convictions and a) being a fretful baby, b) birth abnormalities and c) being withdrawn vs outgoing. However, there are small to moderate positive relationships between juvenile convictions the other predictors in this batch, in particular combined conduct disorder, social handicap and physical neglect (r's ranging between .27 - .33) . There is also a moderate correlation between physical neglect and social handicap (r = .58, which is not surprising, as physical neglect is included in the social handicap composite variable), so we should consider dropping one of them in the model. In addition, there also appears to be a relationship (albeit small-moderate) between social handicap/neglect and conduct disorder, and between mother's attitude and conduct disorder. This could be worth exploring in more detail. It may also be worth including the attitude of only one of the parents, e.g., only mother's attitude, as we expect some overlap there. Now to look at the second batch. 

```{r}
convict.dat %>% 
  select(v28, v91, v105, v125, v155, v297, v119, v158, v300, v301) %>% 
  #Dividing the variables up into 2 chunks to make it easier to read.
  pairs.panels()
```
A quick glance at the chart shows that relatively stronger relationships exist between number of convictions and whether the child was difficult to discipline in school, the child's score on the progressive matrices and verbal comprehension (in short, IQ), and familial criminal record variables (i.e., older sibling and/or parent). Of note is the moderate correlation between the two IQ variables (r = .45), so perhaps we need only include one in the model. Aside from the above-mentioned possible instances of multicollinearity, the predictors appear to be relatively unrelated, so it is unlikely that we will see much "overlapping" explained variance in the model. The two social variables (number of friends and peer-rated popularity) do not appear to account for much variance, suggesting that these variables will be weak predictors. 

I want to further explore the relationship between conduct disorder and parental attitude. 

```{r warning = FALSE, message = FALSE}
convict.dat$v179 <- as.numeric(convict.dat$v179) #coding maternal attitude as a numeric variable for the graph. 
convict.dat$v54 <- as.factor(convict.dat$v54) #changing this variable to a factor so that R will be able to use it as a grouping variable in the graph

ggplot(data = convict.dat, mapping = aes(x = v179, y = v28)) + geom_point(mapping = aes(color = v54)) + geom_smooth(mapping = aes(color = v54)) #Using maternal attitude as the x variable, but the graph looks the same for paternal attitude. 

```
This graph suggests that when conduct disorder is most severe, as maternal attitude go from "good" to "poor" (where "good" = loving normal parenting style and normal disciplinary styles, and "poor" = some combination of harsh, erratic, cruel, neglecting, or disinterested parenting and discipline styles) number of juvenile convictions markedly increases. This in contrast children with less severe/no conduct disorder, where the "jump" in number of convictions is more subtle, when maternal attitude is poor. 

Interestingly, when looking at the second worst level of conduct disorder, the pattern of number of convictions resembles a trigonometric graph! I am not going to try interpret this, but I will note the large margin of error around the interaction lines. Overall, then, it does not seem like there is an interaction effect here. 

Now, I want to look at a possible interaction between social handicap and conduct disorder. 

```{r warning = FALSE, message = FALSE}

convict.dat$v54 <- as.numeric(convict.dat$v54) #coding conduct disorder as a numeric variable for the graph. 
convict.dat$v142 <- as.factor(convict.dat$v142) #coding social handicap as a numeric variable for the graph. 

ggplot(data = convict.dat, mapping = aes(x = v54, y = v28)) + geom_point(mapping = aes(color = v142)) + geom_smooth(mapping = aes(color = v142)) #Using maternal attitude as the x variable, but the graph looks the same for paternal attitude. 


```
There does not appear to be any interesting interaction here. Boys with a greater social handicap are convicted of more crimes regardless of level of conduct disorder; boys with "worse" conduct disorder commit more crimes regardless. What is of note, however, is that the "jump" in the number of convictions when conduct disorder goes from "bad" to "severe" is much greater for boys with a big social handicap. 

QUESTION 3: Build a model/s

Three decisions need to be made prior to building the models. First, do we use the continuous or categorical outcome variable? It might be worth producing both to see how they differ - I think they provide different kinds of information; both are valuable. However, given time and space limitations, I will use the continuous variable, as I find it more interesting.Second, how will we add predictors to the model? Given that we have "categorised" the predictors into themes, it may make sense to attempt a hierarchical method. 

Based on our findings in Q2 above, we can pre-emptively "rule out" a number of predictors:
It did not seem like the birth/infant variables (67, fretful baby, and 113, abnormalities at birth) had much of a relationship with convictions, so I think we can exclude those. In addition the "personality" variable (outgoing vs withdrawn) did not appear to account for a lot of variance in number of convictions. We are only keeping one IQ variable (119, matrices) and are excluding verbal comprehension (158). In the same vein, we can exclude the two social predictors, and only include the teacher rating variables. 

Another important decision to make is how we are going to cross-validate the model. It is worth beginning with a test-train method, especially as we have such a big sample. We may want to look at other methods later in the analysis. 

```{r warning = FALSE, message = FALSE}
set.seed(1)
traindata <- dplyr::sample_frac(convict.dat, 0.70)
testdata <- dplyr::setdiff(convict.dat, traindata)
```

Before buidling the models, we first need to check that all variables are in the correct format (i.e., type). There are lots of variables that are scored as ordered factors (likert-type scale responses), but for present purposes, it makes sense to code them as numeric variables. There are some limitations to this; R will assume that the difference between a 2 and a 3, for example, is 1, and that this difference is the same as that between the 3 and 4. Most of the variables are currently categorised as "labelled" - I am not sure exactly what that means or what the implications are. I think it best to change them all manually. 

```{r}
traindata$v54 <- as.numeric(traindata$v54) #changing the conduct disorder variable back to numeric variable
str(traindata$v123)
traindata$v123 <- as.factor(traindata$v123) #changing the neglect variable into a two level factor. 
str(traindata$v134)
traindata$v134 <- as.factor(traindata$v134) #broken home variable
str(traindata$v123)
traindata$v123 <- as.factor(traindata$v123) #physical neglect
traindata$v91 <- as.numeric(traindata$v91) #interest in child's education
traindata$v142 <- as.numeric(traindata$v142) #social handicap

```

Now to build a model with the continuous variable.

```{r}
cont_model1 <- lm(v28 ~ v54, data = traindata) #starting with only conduct disorder as a predictor
summary(cont_model1)
```
Presence of conduct disorder accounts for 13% of variance in number of convictions. As expected, the combined conduct disorder variable significantly predicts number of convictions. Let's add home/family related variables. 

```{r}
cont_model2 <- lm(v28 ~ v54 + v123 + v142 + v134 + v179 + v91, data = traindata)
summary(cont_model2)
```
The addition of the five home-related variables add 6% of explained variance. Parents' interest in child's education seems to be a contributor here, as does social handicap, having just missed the p = 0.5 convention "cutoff". Looking at the adjusted R squared value, we can already see that we are being penalised for having lots of predictors with not a lot of predictive power (adjusted R squared < R squared by quite a bit). Although this is not strictly in line with model building "rules", we remove the non-significant before looking at the rest.  

```{r}
cont_model3 <- lm(v28 ~ v54 + v142 + v134 + v155 + v297 + v119 +v300 + v301, data = traindata)
summary(cont_model3)
```
Now that we have added the school variables, the social handicap variable is no longer contributing much to the model by way of explaining variance in the outcome variable. Now, only conduct disorder and teacher's rating of difficulty in disciplining are significant predictors, with delinquent older sibling borderline. So, our final model is:

```{r}
cont_final_model <- lm(v28 ~ v54 + v297 +v301, data = traindata)
summary(cont_final_model)
```
This model does not explain a fortune of variance (only 16%, slightly more than conduct disorder by itself), but the model can significantly predict the number of crimes committed by a boy in the sample. The standard error of estimate is also smaller than the SD of the outcome variable (.62 vs .74), so our model is doing a slightly better job than if we had to be making predictions from a null model (using M and SD of the outcome variable).

We can look at the coefficient estimates and calculate the confidence intervals around these parameters using the given standard errors. For example, the unstandardised coefficient for conduct disorder is .16 [.09, .24]; as conduct disorder goes from "good" (no conduct disorder) to "worse" (severe conduct disorder), a boy's score on the conviction variable is expected to increase by .16 points. On this note, a limitation of the outcome variable is that it does not capture exact (raw) number of convictions; in that, for example, a score of 3 could indicate that a boy committed two OR three crimes. Hence, we can only accurately speak about how the predictors influence a boy's score on the conviction variable, and not on the number of convictions per se. Also important to note: these coefficients are unstandardised, so we cannot compare their magnitudes. 

It is worth noting that the combined conduct disorder here includes a teacher's rating of bad behaviour, in addition to the researcher's rating. Hence, two predictors in our final model involve some kind of teacher rating, suggesting that a boy's teacher may be a valuable source of information when determining high-risk children for an intervention, for example. 

Let's plot the model:

```{r message = FALSE}
ggplot(cont_final_model, aes(x = v54 + v297 + v301, y = v28)) + geom_point() + geom_smooth(method = "lm")
```

It is difficult to say much about the graph - the rather discrete nature, and coding similarity, of our predictors and outcome variable does not make for a very interesting plot (i.e., difficult to visualise patterns). As expected, though, there are a number of outliers. Let us take a closer look at some model diagnostics.

First, a quick inspection of partial plots:

```{r}
avPlots(cont_final_model)
```
The avPlots shown above look at the partial relationships between the predictors and the outcome variable. None of the variables show a strong linear relationship, which we already suspected.

Now onto the residuals:

```{r}
qqPlot(cont_final_model)
```
This does not look good - residuals are falling way out of the 95% confidence intervals, and do not look even remotely normally distributed. Are the residual terms correlated?

```{r}
durbinWatsonTest(cont_final_model, max.lag = 1, simulate = TRUE, reps = 1000, method = c("resample","normal")) #checking for independence of residuals

```
No, the DW test is not signficiant, so let us explore further for problems. 

```{r}
plot(cont_final_model)
```
The residuals vs fitted values plot shows a number of stark outlier residuals (the scale-location plot tells us something similar). The leverage residual graph shows a few possible "problematic" cases that may be exerting influence on the model. Let's explore residuals and influential cases further. 

```{r}
ncvTest(cont_final_model) 
```
The test for non-constant error variance is significant, meaning that the residuals, or more accurately, the spread of residuals, are not constant at each level of any predictor in the model. This indicates that our model is biased. This is not entirely surprising, because we know that at the very least, our outcome variable is very skewed. 

```{r}

st.red <- rstandard(cont_final_model) #finding standardised residuals for the model

sort(st.red) > abs(2) #checking for residuals above 2. We find 16
sort(st.red) > abs(3) #find 7 st. residuals over 3

print(sort(st.red)) #checking exact cases

outlierTest(cont_final_model)

```
There are sixteen residuals greater than 2, which amounts to 5.55% of the residuals (just above what we would expect by chance). Three residuals are greater than 3 amounts to 1.04%. Upon closer inspection of the residuals, there is also one above 4, which is a clear residual outlier. If we go back to the raw data to look at these cases (to take a subset, 158, 411, 121, 143, 65, 252, 53, 402). The cases with residuals > 3 are all cases with high conviction score and bad to severe conduct disorder, which is "abnormal" or rare in the sample. The 402 case with the residual above 4, is an interesting case, with the highest possible scoring on convictions, but with mild conduct disorder and does not have a delinquent older sibling. An outlier test from package car confirms that cases 53 and 402 are outliers. Let us check whether this unusual point is exerting undue influence on the model. 

```{r}
cooks <- sort(cooks.distance(cont_final_model))
print(cooks > 1)

hatvalues <- hatvalues(cont_final_model)
sort(hatvalues) 

leveragePlots(cont_final_model)
influencePlot(cont_final_model)
```
There are no points with a Cook's distance above 1, and while there are some hat values above the threshold of 3(k + 1)/n = .04,  but hese points do not necessarily exert influence on the regression coefficients because they are measured on outcome variable rather than the predictors, which we already know is skewed by outliers in the sample. We've had this information before, but the leverage and influence plots provide a nice, clear visual chart showing cases that appear to leveraging the linear model. 

So far, things are not looking good. As a last attempt at redemption, we want to see if our model is generalisable to another sample. We look at the test data set. 

```{r}

test_model <- lm(v28 ~ v54 + v297 + v301, data = testdata)
summary(test_model)

```
Quite different to the train model. Teacher's disciplinary rating is not significant, and the R squared is much smaller than for the train model. SEE is also larger. 

Let's compare their mean squared errors:

```{r}
mean_squared_error <- function(x, y) {
  return(list("training" = mean(resid(x)^2), "test" = mean(resid(y)^2)))
}

mean_squared_error(cont_final_model, test_model)

```
As expected, the mean squared error for the test model is somewhat higher than for the train model. So, not only is our model not good, but it is also quite biased. 

We reach a bit of a dead end here, I think. The aim of this analysis is to determine criminality in children. We managed to diagnose a few influential cases and residual outliers, but these cases that may be contributing to a "problematic" model are the cases where juvenile convictions were recorded. We cannot exclude them, but then our model is both biased and bad (although, Box does concede that all models are, in fact, wrong). The question is, is our model useful? I can guess that the answer to that question is "no", but we want to try determine how accurate the model is at predicting juvenile convictions anyway.

We want to know the positive and negative predictive values of the model (i.e., the sensitivity and specificity). Can the model successfully predict which boys will commit crimes as minors? Conversely, can it predict which boys will not be convicted as minors?

The formula for the true positive is as follows: a/a + c where a is the number of boys who were correctly predicted to commit a crime, and c is the number of boys who committed a crime but were not correctly identified by the model. So, a + c is the total number of boys who committed a crime.  

To calculate true negative, we need to find d/d + b, where d is the number of boys who were correctly identified as having committed a crime, and b is the number of boys who did not commit any crime, but whom the model falsely identified as being a criminal (false positive). 

How are we going to calculate this in R? 

We need to create a variable consisting of the predicted values for the model and then add it to our dataset. Our function will need to find how many times an observed score of 1 (no convictions) matches a predicted value of 1 (true negative). We expect this, at least, to be quite good, because the majority of the sample were not convicted. We also need the function to determine the number of times an observed value of > 1 matches a predicted value of > 1 (true positive). We also need to know the balances, i.e., the number of participants whose conviction score was incorrectly predicted either way, to calculate the sensitivity and specificity. 

```{r}

predict_val <- round(predict(cont_final_model)) #rounded off predicted values from our model because we'll need to compare actual and predicted scores.
length(predict_val)

obs <- traindata$v28 #actual values
length(obs)

```

Problem - two variables have different lengths. This is because the model will not include data points in the model if there are NAs. So, we need to find the problematic variable(s) and delete those cases, and then join the column of predicted values to a dataset. We find that the variable with missing data is the teacher discipline one, with 32 NAs.

```{r}

sum(is.na(traindata$v297)) #teacher discipline variable has 32 cases missing, so we need to remove these. 

new_train <- traindata %>% 
  select(v28, convicted_as_juvenile, v54, v297, v301) %>% 
  na.omit() #get rid of NAs

length(new_train$v28)== length(predict_val) #True.

predict_train <- cbind(new_train, predict_val) #join to the dataset.

```

Now to determine whether the actual and predicted values were identical. 

```{r}

d <- predict_train %>% 
  dplyr::filter(v28 == 1 & predict_val == 1) %>% #True negative: When obs is 1 and pred is 1
  nrow() #175 is true negative

a <- predict_train %>% 
  dplyr::filter(v28 > 1 & predict_val > 1) %>%  #True positive: When obs is not/> 1 and so is pred
  nrow() #20 is true positive

b <- predict_train %>% 
  dplyr::filter(v28 == 1 & predict_val != 1) %>%  #False positive: When obs is 1 but pred is not/>
  nrow() #29 is false positive

c <- predict_train %>% 
  dplyr::filter(v28 > 1 & predict_val == 1) %>% #False negative: When obs is not/> 1 but pred is 1
  nrow() #32 is true negative

#All adds up to 256, which is great. 

#Now we can calculate our true positives and true negatives!

sens <- a/(a+c)
print(sens)

spec <- d/(d+c)
print(spec)

```

As predicted, sensiivity is high (84%) and specificity is low (38%). Let's turn this into a function. 

```{r}

accuracy <- function(df, act_column, pred_column) {
d <- df %>% 
  dplyr::filter(act_column == 1 & pred_column == 1) %>% 
  nrow() #true negative
a <- df %>% 
  dplyr::filter(act_column > 1 & pred_column > 1) %>% 
  nrow() #true positive
b <- df %>% 
  dplyr::filter(act_column == 1 & pred_column != 1) %>% 
  nrow() #false positive
c <- df %>% 
  dplyr::filter(act_column > 1 & pred_column == 1) %>% 
  nrow() #true negative
se <- round((a/(a+c)), digits = 2)
print(paste("Sensitivity is", se))
sp <- round((d/(d+c)), digits = 2)
print(paste("Specificity is", sp)) }

accuracy(predict_train, predict_train$v28, predict_train$predict_val) #It works. 

```

Some limitations of this function:

1. It is limited to the filter conditions used for this analysis. Hence, it would only be useful to someone working on this dataset, using this outcome variable, and answering this type of research question. 

2. There are no error checking messages (due to time constraints). For example, it would not work if there was missing data, etc. It would be great if the function could prompt the user to any problems in the data. 

Limitations of this analysis in general:

1. It would have been useful to look at bootstrapped regression parameters, such as 95% confidence intervals around the regression coefficients. Sadly, I ran out time for that. 

2. I also did not perform any transformations on the skewed variables; some further exploration and trial-and-error there may have shed light on why the model was not "behaving" well. 

3. A more thorough, systematic exploration of the predictor variables might have allowed us to create a model with stronger predictive power (again, time contraints!). 

4. GITHUB REPO LINK

https://github.com/MichalZieff/R_Test1.git













